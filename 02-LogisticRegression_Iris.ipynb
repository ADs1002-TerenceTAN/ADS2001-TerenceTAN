{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lesson we will discuss the logistic regression classifier model. As will be seen, this is closely related to linear regression. We will use the [Iris dataset](https://en.wikipedia.org/wiki/Iris_flower_data_set), which is built in to `seaborn` and `sklearn`. \n",
    "\n",
    "We will firstly explain how logistic regression can be used to classify binary data, by considering just one species of iris. We will then explain how the accuracy of logistic regression can be evaluated using a range of methods. The exercise will then construct a simple multiclass regression.\n",
    "\n",
    "First, import various libraries which we require."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns;\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now import the iris data set and undertake some exploratory data analysis. The data frame has 150 rows and 5 columns. Viewing the header shows there are four features: sepal_length, sepal_width, petal_length and petal_width. These are all in cm. There is one target variable, which is the species of iris."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the iris dataset is (150, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width species\n",
       "0           5.1          3.5           1.4          0.2  setosa\n",
       "1           4.9          3.0           1.4          0.2  setosa\n",
       "2           4.7          3.2           1.3          0.2  setosa\n",
       "3           4.6          3.1           1.5          0.2  setosa\n",
       "4           5.0          3.6           1.4          0.2  setosa"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris = sns.load_dataset('iris') # load the dataset from seaborn\n",
    "print('Shape of the iris dataset is',iris.shape) # display the shape of the data\n",
    "iris.head() # display the first few lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By viewing the statistics we see that there are no missing entries and the standard deviation of each column is fairly similar. However, we will normalise the data before modelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5.843333</td>\n",
       "      <td>3.057333</td>\n",
       "      <td>3.758000</td>\n",
       "      <td>1.199333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.828066</td>\n",
       "      <td>0.435866</td>\n",
       "      <td>1.765298</td>\n",
       "      <td>0.762238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.300000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>5.100000</td>\n",
       "      <td>2.800000</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.800000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.350000</td>\n",
       "      <td>1.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.400000</td>\n",
       "      <td>3.300000</td>\n",
       "      <td>5.100000</td>\n",
       "      <td>1.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>7.900000</td>\n",
       "      <td>4.400000</td>\n",
       "      <td>6.900000</td>\n",
       "      <td>2.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sepal_length  sepal_width  petal_length  petal_width\n",
       "count    150.000000   150.000000    150.000000   150.000000\n",
       "mean       5.843333     3.057333      3.758000     1.199333\n",
       "std        0.828066     0.435866      1.765298     0.762238\n",
       "min        4.300000     2.000000      1.000000     0.100000\n",
       "25%        5.100000     2.800000      1.600000     0.300000\n",
       "50%        5.800000     3.000000      4.350000     1.300000\n",
       "75%        6.400000     3.300000      5.100000     1.800000\n",
       "max        7.900000     4.400000      6.900000     2.500000"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.describe() # show the statistics of the numerical columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target field is a string, and has three unique values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['setosa', 'versicolor', 'virginica'], dtype=object)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris['species'].unique() # show the unique values of the species column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the classification against the first two features, it is apparent that setosa is fairly well separated in feature space, whereas versicolor and virginica have a blurred boundary. Note that the boundary may differ using other features. We would expect that classifiers should be able to easily classify setosa, though the other two species may be a bit more difficult. You can experiment boundaries by changing the features or using `sns.pairplot`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x7fa1a97c1100>"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# plot the first two fields of the iris dataset and classify the points based on the species\n",
    "# fit_reg=False indicates don't fit a regression model\n",
    "# hue='species' indicates to classify the points based on the species field\n",
    "sns.lmplot(data=iris,x=\"sepal_length\",y=\"sepal_width\",hue='species',fit_reg=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this is a multiclass classification problem, we need to convert the label column to binary columns. This is known as one-hot encoding. The routine `pd.get_dummies()` converts the `iris.species` field, which has three values, to three binary columns where the names of the columns are the same as the categorical values. Once we create these columns they can be joined to the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>setosa</th>\n",
       "      <th>versicolor</th>\n",
       "      <th>virginica</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   setosa  versicolor  virginica\n",
       "0       1           0          0\n",
       "1       1           0          0\n",
       "2       1           0          0\n",
       "3       1           0          0\n",
       "4       1           0          0"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummies = pd.get_dummies(iris.species) # one-hot encoding\n",
    "dummies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>setosa</th>\n",
       "      <th>versicolor</th>\n",
       "      <th>virginica</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5.843333</td>\n",
       "      <td>3.057333</td>\n",
       "      <td>3.758000</td>\n",
       "      <td>1.199333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.828066</td>\n",
       "      <td>0.435866</td>\n",
       "      <td>1.765298</td>\n",
       "      <td>0.762238</td>\n",
       "      <td>0.472984</td>\n",
       "      <td>0.472984</td>\n",
       "      <td>0.472984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.300000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>5.100000</td>\n",
       "      <td>2.800000</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.800000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.350000</td>\n",
       "      <td>1.300000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.400000</td>\n",
       "      <td>3.300000</td>\n",
       "      <td>5.100000</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>7.900000</td>\n",
       "      <td>4.400000</td>\n",
       "      <td>6.900000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sepal_length  sepal_width  petal_length  petal_width      setosa  \\\n",
       "count    150.000000   150.000000    150.000000   150.000000  150.000000   \n",
       "mean       5.843333     3.057333      3.758000     1.199333    0.333333   \n",
       "std        0.828066     0.435866      1.765298     0.762238    0.472984   \n",
       "min        4.300000     2.000000      1.000000     0.100000    0.000000   \n",
       "25%        5.100000     2.800000      1.600000     0.300000    0.000000   \n",
       "50%        5.800000     3.000000      4.350000     1.300000    0.000000   \n",
       "75%        6.400000     3.300000      5.100000     1.800000    1.000000   \n",
       "max        7.900000     4.400000      6.900000     2.500000    1.000000   \n",
       "\n",
       "       versicolor   virginica  \n",
       "count  150.000000  150.000000  \n",
       "mean     0.333333    0.333333  \n",
       "std      0.472984    0.472984  \n",
       "min      0.000000    0.000000  \n",
       "25%      0.000000    0.000000  \n",
       "50%      0.000000    0.000000  \n",
       "75%      1.000000    1.000000  \n",
       "max      1.000000    1.000000  "
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris = iris.join(dummies) # join the binary columns to the original dataset\n",
    "iris.describe() # show the statistics of the numerical features of the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create a features table XX which comprises the first four columns of the iris data set, and a target series Y which is the binary column we have just created to identify whether or not the species is versicolor. As we will use regularization, we will use the normalized version of the features matrix X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the table XX will have the first 3 columns (0 to 3) of iris, and Y will have the binary classification column\n",
    "XX, Y = iris[iris.columns[:4]], iris.versicolor \n",
    "X = (XX-XX.mean())/XX.std() # create a new feature matrix for analysis which has mean 0 and standard deviation 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we split these data sets into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split # import the splitting method from sklearn\n",
    "\n",
    "# split the data into 80% training and 20% testing, random_state=0 ensures that the results are repeatable\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,Y,train_size=0.8,random_state=0)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now introduce the sigmoid or logistic function. This is a defined as\n",
    "\n",
    "$$ f(x) = \\displaystyle{\\frac{1}{1+e^{-x}}}, $$\n",
    "\n",
    "and is a smooth, one-to-one (every x value gives a unique y value) function with the domain $(-\\infty,\\infty)$ and range $(0,1)$. If x is negative then the logistic function is less than 0.5 but greater than 0, while if x is positive the logistic function is greater than 0.5 but less than 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "xa = np.linspace(-6,6) # create a linear array of x values between -6 and 6\n",
    "plt.plot(xa,1/(1+np.exp(-xa))); # plot the logistic function \n",
    "plt.xlabel(\"x\") # xlabel\n",
    "plt.ylabel(\"y\") # ylabel\n",
    "plt.title(\"Logistic function y = 1/(1+exp(-x))\"); # title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression assumes that the data is described by m instances or points, and at each of these there are n features. At each point from i=1,2,...,m, the probability of the data being classified as true or false is modelled by\n",
    "\n",
    "$$ y_i = f(c_0 + c_1 x_{i,1} + c_2 x_{i,2} + \\cdots + c_n x_{i,n}), $$\n",
    "\n",
    "where $f$ is the **logistic** function. The algorithm then calculates the optimal coefficients $c_1$, $c_2$,..., $c_n$ and the intercept $c_0$. For linear regression the coefficients were calculated by minimizing the sum of the square of the errors. This is equivalent to maximimizing the likelihood of the observed data, assuming the data points are distributed with mean 0 and some standard deviation $\\sigma$. Therefore, the observed data is the most likely data. For logistic regression these two statements are no longer equivalent, however, the coefficients can be calculated by again maximizing the likelihood of the observed data. This is done by using optimization algorithms.\n",
    "\n",
    "The process for logistic regression using `sklearn` is the same as the other models we have so far considered, however the options change when instantiating the model. We will use the default solver `lbfgs` and no regularization, which corresponds to `penalty='none'`. We can then fit the data and show the model partameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model coefficients are [[-0.728 -1.04   3.269 -2.612]]\n",
      "Model intercept is [-1.139]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression # import the LogisticRegression model\n",
    "# instantiate the model (using the default parameters)\n",
    "# penalty='none' implies no regularization and solver='lbfgs' is the default solver\n",
    "# different solvers can be used, dependent on the type of penalties that are implemented\n",
    "logreg = LogisticRegression(solver='lbfgs',penalty='none')\n",
    "logreg.fit(X_train,y_train) # fit the training data to the model\n",
    "print('Model coefficients are',np.round(logreg.coef_,3)) # print the model coefficients c1,...,c4\n",
    "print('Model intercept is',np.round(logreg.intercept_,3)) # print the model intercept c0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we investigate the probabilities that are output. The second column, which is the probability of correctly picking the species, is the output of the logistic function. Since the problem is binary, the first column is just 1 minus the second column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability table for testing set is:\n",
      "[[0.89243578 0.10756422]\n",
      " [0.13005863 0.86994137]\n",
      " [0.98909241 0.01090759]\n",
      " [0.35327204 0.64672796]\n",
      " [0.87792911 0.12207089]\n",
      " [0.91870274 0.08129726]\n",
      " [0.94905456 0.05094544]\n",
      " [0.78248685 0.21751315]\n",
      " [0.53102397 0.46897603]\n",
      " [0.65646548 0.34353452]\n",
      " [0.07941501 0.92058499]\n",
      " [0.8355475  0.1644525 ]\n",
      " [0.27061832 0.72938168]\n",
      " [0.63956782 0.36043218]\n",
      " [0.48313743 0.51686257]\n",
      " [0.90068995 0.09931005]\n",
      " [0.63590785 0.36409215]\n",
      " [0.1914039  0.8085961 ]\n",
      " [0.7974158  0.2025842 ]\n",
      " [0.98978748 0.01021252]\n",
      " [0.7190421  0.2809579 ]\n",
      " [0.6093423  0.3906577 ]\n",
      " [0.74197751 0.25802249]\n",
      " [0.60756692 0.39243308]\n",
      " [0.72457132 0.27542868]\n",
      " [0.95367092 0.04632908]\n",
      " [0.9507414  0.0492586 ]\n",
      " [0.60314947 0.39685053]\n",
      " [0.22360126 0.77639874]\n",
      " [0.92223402 0.07776598]]\n"
     ]
    }
   ],
   "source": [
    "y_preda = logreg.predict_proba(X_test) # calculate the probabilities for the test features\n",
    "# print out the probability table with a header\n",
    "print('Probability table for testing set is:')\n",
    "print(y_preda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a classification threshold of 0.5, i.e., $P>0.5$ indicates true and $P<0.5$ indicates false, the probabilities are then used to predict the target test values, which can be compared against the actual target test values. We can see there are correct predictions at (1,1) (true positives) and (0,0) (true negative), but there are also false positives at (1,0) and false negatives at (0,1). False positives indicates that the model incorrectly predicts a positive value, similarly false negatives indicates that the model incorrectly predicts a negative value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=logreg.predict(X_test) # calculate the predicted values of the model for the test features\n",
    "plt.scatter(y_pred,y_test) # plot the predicted values against the actual test values\n",
    "plt.xlabel('Predicted') # xlabel\n",
    "plt.ylabel('Actual') # ylabel \n",
    "plt.title('Logistic Regression'); # add a title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To determine the number of correct predictions, false positives and false negatives, we can construct a confusion matrix. From the confusion matrix it can be seen that there are 15 true negatives, 5 true positives, 2 false positives where versicolor was incorrectly predicted, and 8 false negatives, where the other species was incorrectly predicted. The false positive and negatives will typically relate to the virginica species, rather than setosa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix # import the confusion matrix function\n",
    "cnf_matrix = confusion_matrix(y_test, y_pred) # create a confusion matrix for our actual and predicted values\n",
    "# create a data frame from the confusion matrix with the column and row names being the class_names\n",
    "class_names=['other', 'versicolor'] # names of the binary classes for plotting\n",
    "cmatrix = pd.DataFrame(cnf_matrix,columns=class_names,index=class_names) \n",
    "f, ax = plt.subplots(figsize=(7,6)) # initialise the plots and axes\n",
    "sns.heatmap(cmatrix, annot=True, linewidths=.5) # plot the confusion matrix as a heatmap\n",
    "plt.title('Confusion matrix') # add a title\n",
    "plt.ylabel('Actual label') # add a ylabel\n",
    "plt.xlabel('Predicted label') # add a xlabel\n",
    "# adjust the bottom and top of the figure, so we can view all of it\n",
    "bottom, top = ax.get_ylim()  # get the y axis limits\n",
    "ax.set_ylim(bottom + 0.5, top - 0.5); # adjust the y axis limits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can calculate various scores to determine the effectiveness of the classifier. The accuracy score is the number of correct scores divided by the number of samples. The precision score is the number of true positives divided by the true positives plus false positives. The recall score is the number of true positives divided by the true positives plus false negatives. The precision and recall scores give an indication of how well the algorithm is able to pick positive samples, whereas the accurary gives an indication of how well the algorithm can predict correct samples overall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.667\n",
      "Precision: 0.714\n",
      "Recall: 0.385\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score # import the score functions \n",
    "print(\"Accuracy:\",np.round(accuracy_score(y_test, y_pred),3)) # calculate and print the accuracy score\n",
    "print(\"Precision:\",np.round(precision_score(y_test, y_pred),3)) # calculate and print the precision score\n",
    "print(\"Recall:\",np.round(recall_score(y_test, y_pred),3)) # calculate and print the recall score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These measures, and other measures of the classification, can also be plotted by using the `sklearn` function `classification_report`. The **f1-score** is a combination of the **precision** and **recall**. This function is particularly useful for multiclass problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.88      0.75        17\n",
      "           1       0.71      0.38      0.50        13\n",
      "\n",
      "    accuracy                           0.67        30\n",
      "   macro avg       0.68      0.63      0.62        30\n",
      "weighted avg       0.68      0.67      0.64        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To further quantify the accurary of the classifier we can construct a Response Operating Characteristic (ROC) curve, which plots the false positive rate (FPR) against the true positive rate (TPR) as the classification threshold is changed. The TPR is the same as the precision score, while the FPR is the number of false positives divided by the false positives plus true negatives. For a perfect classifier we would expect that TPR=1 and FPR=0. We have considered up until this point a classification threshold of 0.5. If the classification threshold is very high (close to 1), then we are not going to flag any positive respones, so TPR=0 and FPR=0. If the classification threshold is very low (close to 0), then we will only flag positive responses and TPR=1 and FPR=1. Hence if our classifier is good, as the threshold changes the ROC curve will hug the left and top boundaries of the figure. If this occurs the area under the ROC curve (AUC) will approach 1. Therefore classifiers can be compared for a particular problem by measuring the AUC. Any classifier which is below the line y=x and has AUC less than 0.5 will be useless. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the functions to calculate the parameters for the ROC curve and the AUC\n",
    "from sklearn.metrics import roc_curve, roc_auc_score \n",
    "y_pred_proba = logreg.predict_proba(X_test)[::,1] # extract the second column of the model probabilities \n",
    "# calculate the false positive and true positive rates as the threshold is varied, we don't use thresholds\n",
    "fpr, tpr, thresholds = roc_curve(y_test,  y_pred_proba)\n",
    "auc = roc_auc_score(y_test, y_pred_proba) # calculate the area under the ROC curve (AUC)\n",
    "# plot the FPR vs TPR and format label with AUC to 3 decimal places\n",
    "plt.plot(fpr,tpr,label=\"Logistic Regression, auc = %0.3f \" % auc)\n",
    "plt.plot([0,1],[0,1],'k--') # plot x = y for comparison\n",
    "plt.plot([0, 0, 1], [0, 1, 1], 'g--', label=\"'Perfect' Classifier\")\n",
    "plt.xlabel('False Positive Rate') # add xlabel\n",
    "plt.ylabel('True Positive Rate') # add ylabel\n",
    "plt.title('ROC curve') # add title\n",
    "plt.legend(loc='best'); # add legend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with Linear Regression we can use regularization on Logistic Regression to improve the accuracy of the model. Here we will only consider Ridge regularization. For Logistic Regression, Ridge regularization is implemented by setting `penalty='l2'` and the regularization is inversely proportional to the parameter C, which has the default value 1. Therefore C very large corresponds to no regularization. In the following we calculate the ROC curve and the corresponding AUC for C=1 and 5. As is apparent we obtain an increase in the AUC with C=5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(fpr,tpr,label=\"LR, auc =  %0.3f \" % auc) # original model with no regularization \n",
    "\n",
    "for CC in  [1.,5.]:\n",
    "# instantiate the logistic regression model with ridge regularization and C=CC\n",
    "    logreg1 = LogisticRegression(solver='lbfgs',penalty='l2',C=CC) \n",
    "    logreg1.fit(X_train,y_train) # fit the data to the model\n",
    "    y_pred = logreg1.predict(X_test) # calculate the probabilites for the test features\n",
    "    y_pred_proba = logreg1.predict_proba(X_test)[::,1] # calculate the probabilites for the test features\n",
    "    fpr1, tpr1, thresholds1 = roc_curve(y_test,  y_pred_proba) # calculate the FPR and TPR for the ROC curve\n",
    "    auc1 = roc_auc_score(y_test, y_pred_proba) # calculate the area under the ROC curve\n",
    "    plt.plot(fpr1,tpr1,label=\"C={:.0f}, auc =  {:.3f} \".format(CC,auc1)) # regularization with given value of C\n",
    "\n",
    "# plot the ROC curves for the three models\n",
    "plt.plot([0,1],[0,1],'k--') # plot x = y for comparision\n",
    "plt.plot([0, 0, 1], [0, 1, 1], 'g--', label=\"'Perfect' Classifier\")\n",
    "plt.xlabel('False Positive Rate') # add xlabel\n",
    "plt.ylabel('True Positive Rate') # add ylabel\n",
    "plt.title('ROC curves with Ridge regularization') # add title\n",
    "plt.legend(loc='best'); # add legend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of this exercise is to construct a simple one vs rest multiclass classifier based on the probabilities for each of the binary problems for the three iris species. For this problem, only use the first two features, i.e., sepal width and sepal length. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1** \n",
    "\n",
    "Create a new features dataframe only using the first two columns of the iris features. (2 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sepal_length  sepal_width\n",
      "0           5.1          3.5\n",
      "1           4.9          3.0\n",
      "2           4.7          3.2\n",
      "3           4.6          3.1\n",
      "4           5.0          3.6\n"
     ]
    }
   ],
   "source": [
    "iris = sns.load_dataset('iris') # load the dataset from seaborn\n",
    "first_two_columns_df = iris.iloc[:, :2]# use only first two columns of the iris dataset\n",
    "print(first_two_columns_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exericse 2** \n",
    "\n",
    "For each of the iris categories, model the data using Logistic Regression with C=5 for Ridge Regularization and calculate the probabilities for the testing set of the category being correct. Store these probabilites in an array. For each category you will need to resplit the data set, but make sure you use the same random state. Alternatively, split the data set initially with all three labels, and then work on each category. The necessary one-hot encoding has already been performed. (3 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.98776476 0.00640613 0.00162437 0.98605231 0.98435249 0.0059803\n",
      "  0.98676565 0.00810487 0.98969389 0.99118497 0.01027591 0.00465039\n",
      "  0.00342557 0.00535735 0.0032052  0.97908921 0.00460883 0.9921478\n",
      "  0.98841562 0.00608251 0.0056155  0.01625016 0.00594216 0.00666532\n",
      "  0.00221074 0.00763612 0.00805623 0.00351514 0.00520853 0.00655105\n",
      "  0.00108917 0.00245864 0.98725082 0.00479761 0.00268719 0.01627861\n",
      "  0.98432417 0.00435171 0.00291628 0.00220164 0.01369836 0.98122198\n",
      "  0.98238948 0.00240577 0.00390943]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Load the iris dataset from seaborn\n",
    "iris = sns.load_dataset('iris')\n",
    "dummies = pd.get_dummies(iris.species) # one-hot encoding\n",
    "iris = iris.join(dummies)\n",
    "iris.describe()\n",
    "iris['species'].unique() \n",
    "X = iris.drop(columns='species')\n",
    "y = iris['species']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "\n",
    "probabilities_array = []\n",
    "\n",
    "\n",
    "logreg = LogisticRegression(solver='lbfgs', penalty='l2', C=5, max_iter=1000) # had to use max_iter as i got a reuslt saying the max iteration was reached. \"STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\"\n",
    "\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred_proba = logreg.predict_proba(X_test)\n",
    "\n",
    "probabilities_array.append(y_pred_proba[::, 1])\n",
    "\n",
    "probabilities_array = np.array(probabilities_array)\n",
    "\n",
    "print(probabilities_array)\n",
    "\n",
    "# probably worng from looking at numbers but i dont know what else to do as when tried splitting it in other ways, it just returned that setosa' and 'versicolor' could not be found in iris even tho i used iris.describe to confirm it was there\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3** \n",
    "\n",
    "For each instance in feature space, choose the category with the highest probability. The function `numpy.argmax()` will be allow you to do this. (3 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n"
     ]
    }
   ],
   "source": [
    "highest = np.argmax(probabilities_array)\n",
    "\n",
    "print(highest)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4** \n",
    "\n",
    "Plot the final classification for the testing set in feature space, with colours based on the predicted category. (2 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=X_test.iloc[:, 0], y=X_test.iloc[:, 1], hue=iris.species[highest], palette='viridis', edgecolor='b')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend(title='final classification for the testing set')\n",
    "plt.show()\n",
    "# yeh i dont know. probably something went wrong with the probability numbets obtained from the array?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
